Sawtooth wave for regularize

	sawtooth(x):=mod(x+%pi,2*%pi)-%pi
	stf(n,x):=2*sum((-1)**(i-1)*sin(i*x)/i, i, 1, n)
	sta(q,x):=2*sin(x)/(2*q*cos(x)+q^2+1)

Oscillations in stf and its derivative are not good for optimizations.

topological charge in 1d,

	qt(x):=sum(sawtooth(x[mod(i+1,n)]-x[i]), i, 0, n-1)
	qf(j,x):=sum(stf(j,x[mod(i+1,n)]-x[i]), i, 0, n-1)
	qa(q,x):=sum(sta(q,x[mod(i+1,n)]-x[i]), i, 0, n-1)

	plot2d(ev(
		[sawtooth(x[2]-x[1])+sawtooth(x[1]-x[0]),
		 stf(3,x[2]-x[1])+stf(3,x[1]-x[0]),
		 sta(0.4,x[2]-x[1])+sta(0.4,x[1]-x[0]),
		 sta(0.5,x[2]-x[1])+sta(0.5,x[1]-x[0])
		], x[0]=1.1, x[2]=0.5
	), [x[1], -2*%pi, 2*%pi])

qa has one artifical local maxima.
qf has many, except for n=1.

	plot2d(ev(''(
		map(lambda([f],diff(f,x[1])),
		ev([qf(3,x), qf(9,x), qa(0.5,x), qa(0.6,x)], n=4))
	), x[0]=0.1, x[2]=0.2), [x[1], -2*%pi, 2*%pi])

qf has many oscillations in the derivative of the topological charge.
qa is smooth.
sin (qf, n=1) is smooth.


Gradient flow style update

	w: -beta*(cos(n*(x-l))+cos(n*(r-x)))/n²
	y: x+diff(w,x)  →  x+beta*(sin(n*(x-l))-sin(n*(r-x)))/n
	f(x):=x+beta*(sin(n*(x-l))-sin(n*(r-x)))/n - beta*(sin(n*l)+sin(n*r))/n  for n odd
	f(x):=x+beta*(sin(n*(x-l))-sin(n*(r-x)))/n + beta*(sin(n*l)+sin(n*r))/n  for n even
	diff(f(x),x)  →  1 + beta*(cos(n*(x-r))+cos(n*(x-l)))

satisfies,

	f(-%pi) = -%pi
	f(%pi) = %pi
	diff(f(x),x) > 0, if -0.5 < beta < 0.5

so f(x) is bijective over domain [-π, π], so is the map

	y: f(x) +/- beta*n*(sin(n*l)+sin(n*r))


Loss function

Cosine difference of plaquettes, p0, and p1
	E[⟨1-cos(p0-p1)⟩]
	= 1 - E[⟨sin(p0) sin(p1)⟩] - E[⟨cos(p0) cos(p1)⟩]
for uncorrelated,
	= 1 - E[⟨cos(p)⟩]²
or
	E[⟨1-cos(p0-p1)⟩]
	= E[⟨2 sin²((p0-p1)/2)⟩]
	= 2 E[⟨sin²((p0-p1)/2)⟩]
where ⟨⟩ configuration average, E[] ensemble expectation value.
For topology, approximation with sine, 1st term in Fourier series,
	E[ ( V ⟨sin(p0)⟩ - V ⟨sin(p1)⟩ )² ]
	= V² E[ ⟨sin(p0)⟩² + ⟨sin(p1)⟩² - 2 ⟨sin(p0)⟩ ⟨sin(p1)⟩ ]
	= V² { E[⟨sin(p0)⟩²] + E[⟨sin(p1)⟩²] - 2 E[⟨sin(p0)⟩ ⟨sin(p1)⟩] }
for uncorrelated
	= 2 V² { E[⟨sin(p)⟩²] - E[⟨sin(p)⟩]² }
	= 2 V² E[⟨sin(p)⟩²]


Luscher's trivializing maps paper, Luscher (2010)
describes field transformations with group manifold.
Here is a summary in terms of linear algebra, matrices and vectors.

Target is the functional integral,
	⟨O⟩ = 1/Z ∫ dx O(x) exp(-S(x))
where we consider x a vector, xₘ.

Change of variable,
	xₖ = Fₖ(y)
with vector functions Fₖ,
	⟨O⟩ = 1/Z ∫ dy |det(J(y))| O(F(y)) exp( -S(F(y)) )
where the Jacobian matrix,
	Jₖₗ(y) = ∂Fₖ(y) / ∂yₗ.
F has to be:

	- Injective (1 to 1), from the new integration domain to the old
	- Continuously differentiable (or differentiable and have continuous inverse)

Rewrite the integral as,
	⟨O⟩ = 1/Z ∫ dy O(F(y)) exp( -S(F(y)) + ln|det(J(y))| )

F is a trivializing map, when
	S(F(y)) - ln|det(J(y))| = constant
and our expectation value simplifies to
	⟨O⟩ = 1/Z ∫ dy O(F(y))

From Luscher:
	Field transformations are invertible maps of this manifold
	onto itself.  Such transformations will always be required to
	be differentiable in both directions and
	orientation-preserving (here and below, “differentiable” means
	“infinitely often continuously differentiable”).

He argues for the existence of a such trivializing map.

In terms of HMC, we add the conjugate momenta to y,
and use the equations of motien derived from the Hamilton function
	ℋ(y,π) = ½π² + S(F(y)) - ln|det(J(y))|

Consider a change of variable in π,
	πₖ = Jₖₗ(y) pₗ = Jₖₗ(F⁻¹(x)) pₗ
we get a Hamilton function
	H(x,p) = ½ p^† M p + S(x) - ln|det(J)|
where the positive definite M,
	Mₖₗ(x) = J^*ₙₖ(F⁻¹(x)) Jₙₗ(F⁻¹(x))
is the kernel of the kinetic term considered long ago by Duane et al
(1986 and 1988).


From
	⟨O⟩ = 1/Z ∫ dx O(x) exp(-S(x))
with a change of variable,
	⟨O⟩ = 1/Z ∫ dy O(F(y)) exp( -S(F(y)) + ln|det(J(y))| )
where
	Jₖₗ(y) = ∂Fₖ(y) / ∂yₗ.
Introducing the conjugate momenta, the Hamilton function becomes,
	ℋ(y,π) = ½π² + S(F(y)) - ln|det(J(y))|
which gives the equation of motion
	d/dt π = -∂/∂y ℋ = - J(y) S'(F(y)) + Tr[ J⁻¹ d/dy(J) ]
	d/dt y =  ∂/∂π ℋ = π
This is separable and can use the usual explicit, symplectic and symmetric
discrete integrators.
Here is the leapfrog as an example,
	π(τ+ε/2) = π(τ)     - ε/2 [ J(y(τ)) S'(F(y(τ))) - Tr[ J⁻¹ d/dy(J(y(τ))) ] ]
	y(τ+ε)   = y(τ)     + ε π(τ+ε/2)
	π(τ+ε)   = π(τ+ε/2) - ε/2 [ J(y(τ+ε)) S'(F(y(τ+ε))) - Tr[ J⁻¹ d/dy(J(y(τ+ε))) ] ]


When the map F(y) = x is a trivializing map,
	S(F(y)) - ln|det(J(y))| = constant
thus,
	d/dt π = -∂/∂y ℋ = J(y) S'(F(y)) - Tr[ J⁻¹ d/dy(J) ] ≈ 0

It is possible to optimize and search for F and J that makes
	∂/∂y ℋ = 0
for all y.

When d/dt π ≈ 0, π stays a constant, while y changes according to
	d/dt y =  ∂/∂π ℋ = π
which makes
	d/dt x = J(y) d/dt y = J(y) π
move through phase space more efficiently.



GENERAL SMEARING

For MD, we have

	U' <- U Exp[dt * ProjectTAH(M)]

where U is covariant and M (being sum of loops) is invariant under gauge transformation.

The most generic form would be

	U(x,mu)' <- ProjectSU Sum_L a_L OrderedProduct[L(x,mu)]

where L(x,mu) is any line connecting gauge links from x to x+mu, a_L is a scalar coefficient.
L(x,mu) may or may not need to go through U(x,mu).
It may also have loops in it, and results in a polynomial of such loop after sum.

The ProjectSU has to satisfy the constraint that

	X ProjectSU(M) Y = ProjectSU(X M Y)

for any X and Y in the group.
I'm not sure how to construct such ProjectSU.

I can, however, rewrite the equation to be similar to the MD update,

	U(x,mu)' <- ProjectSU Sum_L a_L U(x,mu) U(x,mu)^dag OrdProd[L(x,mu)]

such that

	U(x,mu)^dag OrdProd[L(x,mu)]

becomes a complete loop, which is gauge invariant.
We then have it in a simplified form,

	U' <- ProjectSU Sum_R a_R U R

where R is any loop that start and stop at x+mu.

We may write it in terms of

	U' <- ProjectSU Sum_LR a_LR L U R

such that L is any loop that start and stop at x, but since we can always pull the U to the left by multiplying U U^dag, the most generic form is simply,

	U' <- ProjectSU[ U (Sum_R a_R R) ]

where R is any loop that start and stop at x+mu, and may or may not go through U.

I can multiply U U^dag again, so it becomes

	U' <- U { U^dag ProjectSU[ U (Sum_R a_R R) ] }

If we had a ProjectSU that satisfies

	X ProjectSU(M) Y = ProjectSU(X M Y)

the above update would become

	U' <- U ProjectSU[ Sum_R a_R R ]

though we need ProjectSU satisfy a different constraint,

	X ProjectSU(M) X^dag = ProjectSU(X M X^dag)

If I were allowed to do the above change of constraint to ProjectSU, it seems I could just use the projection in MD update,

	U' <- U Exp[ ProjectTAH( Sum_R a_R R ) ]

Question: did we lose any generality in the above derivation?

It looks like we are only one step away from

	U' <- U Exp{ [d/dU] F( ReTr A, ReTr B, ReTr C, ... ) }

where F is any analytical function or neural network, and A,B,C,... are any closed
loops may or may not passing U.

we can simplify it by moving the U independent loops out and keeping
the U dependent loops simple.

	U' <- U Exp{ c atan[F(ReTr X, ReTr Y, ...)] [d/dU] ReTr[W] }

so that F(ReTr X, ReTr Y, ...) only depends on U independent loops,
while W contains U dependent loops, c*atan(F) is for restricting
the Jacobian to be positive definite, and W is a sum of any loop
and its symmetrized versions including U.


Optimize Wilson line computations

Giving path [±d,...] for d ∈ [1,...], we want to optimize the ordered
product computation.

Example:

plaquette: 1 2 -1 -2
L[1,2] ← U[1] U[2](+x1)
L[2,1] ← U[2] U[1](+x2)
L[1,2,-1,-2] ← L[1,2] L[2,1]⁺
3 mul, 2 sft, 2 xfer

rectangle: 1 2 2 -1 -2 -2
L[2,2] ← U[2] U[2](+x2)
L[1,2,2] ← U[1] L[2,2](+x1)
L[2,2,1] ← L[2,2] U[1](+2x2)
L[1,2,2,-1,-2,-2] ← L[1,2,2] L[2,2,1]⁺
4 mul, 3 sft, 4 xfer

Combined: [1 2 -1 -2] [1 2 2 -1 -2 -2]
L[1,2] ← U[1] U[2](+x1)
L[2,1] ← U[2] U[1](+x2)
L[1,2,-1,-2] ← L[1,2] L[2,1]⁺
L[1,2,2] ← L[1,2] U[2](+x1+x2)
L[2,2,1] ← U[2] L[2,1](+x2)
L[1,2,2,-1,-2,-2] ← L[1,2,2] L[2,2,1]⁺
6 mul, 4 sft, 5 xfer < separated 7 mul 5 sft 6 xfer

We are looking for the optimal split,
path = p0 p1 p2 ... pj pk pl ... pz
	← [p0 p1 p2 ... pj] * [pk pl ... pz]

Difficult to recurse down from top, because the optimal split
depends on the optimal split of other paths.
So we start looking from short segments.

0. Find the pair that repeats maximal times in all the paths.
1. Find the next pair that repeats maximal times,
	after fixing those maximal repeating pairs.
2. Find all the pairs with their repeating counts.
3. Looking at pairs of elements that consist of the pairs found
	previously and the left out singletons.

We perform the above iteratively, the type of the path:
	path: (dim, adjoint?) | ((path, ...), adjoint?)
	dim: 0 | 1 | ...
	adjoint?: t | f

The choice of whether to save path segment or its adjoint
depends on how many shifts and transfers we need.

For a tree of operation for path X

X ← L R
	L ← LL LR
		LL ← LLL LLR
		LR ← LRL LRR
	R ← RL RR
		RL ← RLL RLR
		RR ← RRL RRR

if change R to RR RL adj,
	R's location change from RL to RR,
	X's shifts change because of R's location, so do others depending on R


--------

os rc -c 'path=($home/py-ML/bin $path) python -i'
import importlib as il
il.reload(field)
op = field.OrderedPaths(2, ((1,2,-1,-2),(1, 1, 2, 2, -1, -1, -2, -2)))
